{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "torch.set_printoptions(sci_mode = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "reddit_df = pd.read_csv(\"../data/Reddit_Data.csv\")\n",
    "twitter_df = pd.read_csv(\"../data/Twitter_Data.csv\")\n",
    "imdb_df = pd.read_csv(\"../data/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Reddit data\n",
    "reddit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Twitter data\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect IMDB data\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of NaN columns\n",
    "reddit_df = reddit_df.dropna()\n",
    "twitter_df = twitter_df.dropna()\n",
    "imdb_df = imdb_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up IMDB data\n",
    "for i in range(len(imdb_df)):\n",
    "    if imdb_df.iloc[i][1] == \"positive\":\n",
    "        imdb_df.iloc[i][1] = 1\n",
    "    else:\n",
    "        imdb_df.iloc[i][1] = -1\n",
    "\n",
    "    text = \"\".join([char for char in imdb_df.iloc[i][0] if char not in punctuation]).lower()\n",
    "    imdb_df.iloc[i][0] = text;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect IMDB data again\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of comments\n",
    "comments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(reddit_df)):\n",
    "    comments.append(str(reddit_df.iloc[i][0]).lower())\n",
    "  \n",
    "for i in range(len(imdb_df)):\n",
    "    comments.append(str(imdb_df.iloc[i][0]))\n",
    "\n",
    "for i in range(len(twitter_df)):\n",
    "    comments.append(str(twitter_df.iloc[i][0]).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where all the words will stay\n",
    "all_words = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for string in comments:\n",
    "    all_words += string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurences of each word\n",
    "all_words = all_words.split()\n",
    "count_words = Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort them by the most common\n",
    "total_words = len(all_words)\n",
    "sorted_words = count_words.most_common(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {word : i + 1 for i, (word, count) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../data/vocab_to_int\", vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode comments\n",
    "comments_int = []\n",
    "for comment in comments:\n",
    "    try:\n",
    "        result = [vocab_to_int[word] for word in comment.split()]\n",
    "    except:\n",
    "        pass\n",
    "    comments_int.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels (0 = NEGATIVE; 0.5 = NEUTRAL; 1 = POSITIVE)\n",
    "ys = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(reddit_df)):\n",
    "    ys.append((int(reddit_df.iloc[i][1]) + 1) / 2)\n",
    "\n",
    "for i in range(len(imdb_df)):\n",
    "    ys.append((int(imdb_df.iloc[i][1]) + 1) / 2)\n",
    "\n",
    "for i in range(len(twitter_df)):\n",
    "    ys.append((int(twitter_df.iloc[i][1]) + 1) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If comments are shorted than a given sequence\n",
    "# length, pad with zeros\n",
    "# If comments are longer than a given sequence\n",
    "# length, truncate them\n",
    "def pad_features(comments_int, seq_length):\n",
    "    # xs = Matrix(len(comments_int) X seq_length)\n",
    "    xs = np.zeros((len(comments_int), seq_length), dtype = int)\n",
    "    \n",
    "    # For each indexed comment\n",
    "    for i, comment in enumerate(comments_int):\n",
    "        comment_len = len(comment)\n",
    "        \n",
    "        # If the comment length is <= the sequence length\n",
    "        if comment_len <= seq_length:\n",
    "            # Pad with zeros\n",
    "            zeros = list(np.zeros(seq_length - comment_len))\n",
    "            new = zeros + comment\n",
    "        \n",
    "        # Otherwise, truncate\n",
    "        elif comment_len > seq_length:\n",
    "            new = comment[0:seq_length]\n",
    "            \n",
    "        # Populate the features array\n",
    "        xs[i][:] = np.array(new)\n",
    "        \n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 150\n",
    "xs = pad_features(comments_int, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_percentage = 0.7\n",
    "x_train = xs[0 : int(split_percentage * len(xs))]\n",
    "y_train = ys[0 : int(split_percentage * len(ys))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = xs[int(split_percentage * len(xs)):]\n",
    "y_valid = ys[int(split_percentage * len(ys)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataloaders ready for training and testing\n",
    "valid_data = TensorDataset(torch.from_numpy(x_valid), torch.from_numpy(y_valid))\n",
    "train_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, shuffle = True, batch_size = BS, drop_last = True)\n",
    "valid_loader = DataLoader(valid_data, shuffle = True, batch_size = BS, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "class SentimentAnalyzer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        output_size,\n",
    "        embedding_dim,\n",
    "        hidden_dim,\n",
    "        n_layers\n",
    "    ):\n",
    "        super(SentimentAnalyzer, self).__init__()\n",
    "        \n",
    "        # Output layer size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Number of LSTM layers\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Hidden layer dimension of LSTM cell\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            n_layers,\n",
    "            dropout = 0.5,\n",
    "            batch_first = True\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        # Xs are going to be of size BATCH_SIZE x SEQ_LEN\n",
    "        # We are interested in the batch size, so x.size(0)\n",
    "        # will give us that\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Output from the embedding layer\n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        # Output and new hidden layer from\n",
    "        # LSTM layer\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        # We'll convert that LSTM output into a contiguous\n",
    "        # tensor. This prevents certain runtime errors\n",
    "        # from occuring when certain operations happen\n",
    "        # (like getting a view of a transposed tensor)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # Output after dropping some neurons with\n",
    "        # 30% dropout rate\n",
    "        out = F.dropout(lstm_out, 0.3)\n",
    "        \n",
    "        # Output of fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # Our labels are 0, 0.5 and 1\n",
    "        # sigmoid(x) belongs to the interval [-1, 1]\n",
    "        sig_out = F.sigmoid(out)\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:,-1]\n",
    "        \n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(), \n",
    "                 weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate model and hyperparams\n",
    "vocab_size = len(vocab_to_int) + 1\n",
    "output_size = 1\n",
    "embedding_dim = 200\n",
    "hidden_dim = 64\n",
    "n_layers = 2\n",
    "\n",
    "model = SentimentAnalyzer(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "model = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "cnt = 0\n",
    "print_every = 100\n",
    "gradient_clip = 5\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    # Initialize hidden state with \n",
    "    # initial model weights and biases\n",
    "    # Needed because LSTMs are recurrent networks,\n",
    "    # Meaning that each output produces a new\n",
    "    # hidden state, which will be passed though a\n",
    "    # new cell / layer, which will repeat the process.\n",
    "    hidden_state = model.init_hidden(BS)\n",
    "    \n",
    "    # Loop on batch\n",
    "    for x, y in train_loader:\n",
    "        cnt += 1\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        hidden_state = tuple([state.data for state in hidden_state])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        x = x.type(torch.cuda.LongTensor)\n",
    "\n",
    "        out, hidden_state = model(x, hidden_state)\n",
    "        loss = F.binary_cross_entropy(out.squeeze(), y.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        # print(f\"LOSS: {loss.item()} | FIRST 5: {out[:5]}\")\n",
    "\n",
    "        # RNNs and LSTMs have an issue where the gradient will\n",
    "        # explode A.K.A they will get so big or so small, to\n",
    "        # the point where they will overflow the data type's allocated\n",
    "        # (e.g. the would get over the max integer value - 2,147,483,647 - if the gradient\n",
    "        # was an integer)\n",
    "        # PyTorch has a nice util called \"clip_grad_norm\" which can help\n",
    "        # prevent this issue\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (cnt - 1) % print_every == 0:\n",
    "            # Get validation loss\n",
    "            # Same spiel as above, except this is for validation\n",
    "            val_h_s = model.init_hidden(BS)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for x_val, y_val in valid_loader:\n",
    "                val_h_s = tuple([state.data for state in val_h_s])\n",
    "                \n",
    "                x_val, y_val = x_val.cuda(), y_val.cuda()\n",
    "                \n",
    "                x_val = x_val.type(torch.cuda.LongTensor)\n",
    "                \n",
    "                out, val_h_s = model(x_val, val_h_s)\n",
    "\n",
    "                val_loss = F.binary_cross_entropy(out.squeeze(), y_val.float())\n",
    "                \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            print(f\"Epoch: {epoch + 1}/{epochs}\",\n",
    "                  f\"Step: {cnt}\",\n",
    "                  f\"Loss: {loss.item():.6f}\",\n",
    "                  f\"Val Loss: {np.mean(val_losses):.6f}\",\n",
    "                   \"Saving...\")\n",
    "            torch.save(model, \"../models/SentimentAnalyzer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bit93e3f1eb3f6e4d4bafd0e7f31c734b86"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
